{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the path of your jupyter notebook is not set correctly, \n",
    "# uncomment and run with your project path:\n",
    "\n",
    "# import os\n",
    "# os.chdir('/home/sergiu/PycharmProjects/moc1-aset-project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download the raw competition data you can:\n",
    "1. Use the kaggle api:  \n",
    "    1. Consult https://github.com/Kaggle/kaggle-api/issues/15#issuecomment-374432095 to add the api keys. \n",
    "    2. pip install kaggle.\n",
    "    3. kaggle competitions download -c freesound-audio-tagging-2019.\n",
    "2. Call \"wget https://www.kaggle.com/c/10700/download-all\", but the download is limited to 1.8 MB/s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/sergiu/anaconda3/lib/python3.7/site-packages (1.5.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/sergiu/anaconda3/lib/python3.7/site-packages (from kaggle) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil in /home/sergiu/anaconda3/lib/python3.7/site-packages (from kaggle) (2.8.0)\n",
      "Requirement already satisfied: tqdm in /home/sergiu/anaconda3/lib/python3.7/site-packages (from kaggle) (4.32.1)\n",
      "Requirement already satisfied: certifi in /home/sergiu/anaconda3/lib/python3.7/site-packages (from kaggle) (2019.6.16)\n",
      "Requirement already satisfied: python-slugify in /home/sergiu/anaconda3/lib/python3.7/site-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/sergiu/anaconda3/lib/python3.7/site-packages (from kaggle) (1.12.0)\n",
      "Requirement already satisfied: requests in /home/sergiu/anaconda3/lib/python3.7/site-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/sergiu/anaconda3/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/sergiu/anaconda3/lib/python3.7/site-packages (from requests->kaggle) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/sergiu/anaconda3/lib/python3.7/site-packages (from requests->kaggle) (3.0.4)\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/sergiu/.kaggle/kaggle.json'\n",
      "Downloading freesound-audio-tagging-2019.zip to /home/sergiu/PycharmProjects/moc1-aset-project\n",
      "  0%|                                      | 49.0M/24.4G [00:05<47:15, 9.21MB/s]^C\n",
      "  0%|                                      | 50.0M/24.4G [00:06<53:53, 8.07MB/s]\n",
      "User cancelled operation\n",
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!kaggle competitions download -c freesound-audio-tagging-2019\n",
    "# !wget https://www.kaggle.com/c/10700/download-all\n",
    "\n",
    "!mkdir data\n",
    "!mv freesound-audio-tagging-2019.zip data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = 'data/freesound-audio-tagging-2019.zip'\n",
    "import zipfile\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')\n",
    "    os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freesound-audio-tagging-2019.zip  train_curated.csv  train_noisy.zip\r\n",
      "sample_submission.csv\t\t  train_curated.zip\r\n",
      "test.zip\t\t\t  train_noisy.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = ['train_curated', 'train_noisy', 'test']\n",
    "for zip_name in zips:\n",
    "    zip_path = f'data/{zip_name}.zip'\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(f'data/{zip_name}/')\n",
    "        os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is from:  \n",
    "https://www.kaggle.com/daisukelab/cnn-2d-basic-solution-powered-by-fast-ai?fbclid=IwAR35ItZNHdxy1SsAMrz_KLHoQP_DJhXbaf24K9_FQop9p9ggMb-iuPmiqNg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import IPython\n",
    "import IPython.display\n",
    "import PIL\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File/folder definitions:\n",
    "    - df will handle training data.\n",
    "    - test_df will handle test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('../input')\n",
    "CSV_TRN_CURATED = DATA/'train_curated.csv'\n",
    "CSV_TRN_NOISY = DATA/'train_noisy.csv'\n",
    "CSV_SUBMISSION = DATA/'sample_submission.csv'\n",
    "TRN_CURATED = DATA/'train_curated'\n",
    "TRN_NOISY = DATA/'train_noisy'\n",
    "TEST = DATA/'test'\n",
    "\n",
    "WORK = Path('work')\n",
    "IMG_TRN_CURATED = WORK/'image/trn_curated'\n",
    "IMG_TRN_NOISY = WORK/'image/train_noisy'\n",
    "IMG_TEST = WORK/'image/test'\n",
    "for folder in [WORK, IMG_TRN_CURATED, IMG_TRN_NOISY, IMG_TEST]: \n",
    "    Path(folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(CSV_TRN_CURATED)\n",
    "test_df = pd.read_csv(CSV_SUBMISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def read_audio(conf, pathname, trim_long_data):\n",
    "    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n",
    "    # trim silence\n",
    "    if 0 < len(y): # workaround: 0 length causes error\n",
    "        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n",
    "    # make it unified length to conf.samples\n",
    "    if len(y) > conf.samples: # long enough\n",
    "        if trim_long_data:\n",
    "            y = y[0:0+conf.samples]\n",
    "    else: # pad blank\n",
    "        padding = conf.samples - len(y)    # add padding at both ends\n",
    "        offset = padding // 2\n",
    "        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n",
    "    return y\n",
    "\n",
    "def audio_to_melspectrogram(conf, audio):\n",
    "    spectrogram = librosa.feature.melspectrogram(audio, \n",
    "                                                 sr=conf.sampling_rate,\n",
    "                                                 n_mels=conf.n_mels,\n",
    "                                                 hop_length=conf.hop_length,\n",
    "                                                 n_fft=conf.n_fft,\n",
    "                                                 fmin=conf.fmin,\n",
    "                                                 fmax=conf.fmax)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram\n",
    "\n",
    "def show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n",
    "    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n",
    "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
    "                            fmin=conf.fmin, fmax=conf.fmax)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n",
    "    x = read_audio(conf, pathname, trim_long_data)\n",
    "    mels = audio_to_melspectrogram(conf, x)\n",
    "    if debug_display:\n",
    "        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n",
    "        show_melspectrogram(conf, mels)\n",
    "    return mels\n",
    "\n",
    "\n",
    "class conf:\n",
    "    # Preprocessing settings\n",
    "    sampling_rate = 44100\n",
    "    duration = 2\n",
    "    hop_length = 347*duration # to make time steps 128\n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "    n_mels = 128\n",
    "    n_fft = n_mels * 20\n",
    "    samples = sampling_rate * duration\n",
    "\n",
    "# example\n",
    "x = read_as_melspectrogram(conf, TRN_CURATED/'0006ae4e.wav', trim_long_data=False, debug_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Scale to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "def convert_wav_to_image(df, source, img_dest):\n",
    "    X = []\n",
    "    for i, row in tqdm_notebook(df.iterrows()):\n",
    "        x = read_as_melspectrogram(conf, source/str(row.fname), trim_long_data=False)\n",
    "        x_color = mono_to_color(x)\n",
    "        X.append(x_color)\n",
    "    return X\n",
    "\n",
    "X_train = convert_wav_to_image(df, source=TRN_CURATED, img_dest=IMG_TRN_CURATED)\n",
    "X_test = convert_wav_to_image(test_df, source=TEST, img_dest=IMG_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.vision.data import *\n",
    "import random\n",
    "\n",
    "CUR_X_FILES, CUR_X = list(df.fname.values), X_train\n",
    "\n",
    "def open_fat2019_image(fn, convert_mode, after_open)->Image:\n",
    "    # open\n",
    "    idx = CUR_X_FILES.index(fn.split('/')[-1])\n",
    "    x = PIL.Image.fromarray(CUR_X[idx])\n",
    "    # crop\n",
    "    time_dim, base_dim = x.size\n",
    "    crop_x = random.randint(0, time_dim - base_dim)\n",
    "    x = x.crop([crop_x, 0, crop_x+base_dim, base_dim])    \n",
    "    # standardize\n",
    "    return Image(pil2tensor(x, np.float32).div_(255))\n",
    "\n",
    "vision.data.open_image = open_fat2019_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class\n",
    "\n",
    "\n",
    "# Wrapper for fast.ai library\n",
    "def lwlrap(scores, truth, **kwargs):\n",
    "    score, weight = calculate_per_class_lwlrap(to_np(truth), to_np(scores))\n",
    "    return torch.Tensor([(score * weight).sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=True, max_rotate=0, max_lighting=0.1, max_zoom=0, max_warp=0.)\n",
    "src = (ImageList.from_csv(WORK/'image', Path('../../')/CSV_TRN_CURATED, folder='trn_curated')\n",
    "       .split_by_rand_pct(0.2)\n",
    "       .label_from_df(label_delim=',')\n",
    ")\n",
    "data = (src.transform(tfms, size=128)\n",
    "        .databunch(bs=64).normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, models.resnet18, pretrained=False, metrics=[lwlrap])\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-1)\n",
    "learn.fit_one_cycle(10, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(50, slice(1e-3, 3e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, slice(1e-4, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-visualize-the-actual-convolution-filters-in-cnn/13850\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "def visualize_first_layer(learn, save_name=None):\n",
    "    conv1 = list(learn.model.children())[0][0]\n",
    "    if isinstance(conv1, torch.nn.modules.container.Sequential):\n",
    "        conv1 = conv1[0] # for some models, 1 layer inside\n",
    "    weights = conv1.weight.data.cpu().numpy()\n",
    "    weights_shape = weights.shape\n",
    "    weights = minmax_scale(weights.ravel()).reshape(weights_shape)\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(8,8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(np.rollaxis(weights[i], 0, 3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    if save_name:\n",
    "        fig.savefig(str(save_name))\n",
    "\n",
    "visualize_first_layer(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fat2019_fastai_cnn2d_stage-2')\n",
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_X_FILES, CUR_X = list(test_df.fname.values), X_test\n",
    "\n",
    "test = ImageList.from_csv(WORK/'image', Path('../..')/CSV_SUBMISSION, folder='test')\n",
    "learn = load_learner(WORK/'image', test=test)\n",
    "preds, _ = learn.TTA(ds_type=DatasetType.Test) # <== Simply replacing from learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[learn.data.classes] = preds\n",
    "test_df.to_csv('submission.csv', index=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_X_FILES, CUR_X = list(df.fname.values), X_train\n",
    "learn = cnn_learner(data, models.resnet18, pretrained=False, metrics=[lwlrap])\n",
    "learn.load('fat2019_fastai_cnn2d_stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_X_FILES, CUR_X = list(df.fname.values), X_train\n",
    "learn = cnn_learner(data, models.resnet18, pretrained=False, metrics=[lwlrap])\n",
    "learn.load('fat2019_fastai_cnn2d_stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson6-pets-more.ipynb\n",
    "from fastai.callbacks.hooks import *\n",
    "\n",
    "def visualize_cnn_by_cam(learn, data_index):\n",
    "    x, _y = learn.data.valid_ds[data_index]\n",
    "    y = _y.data\n",
    "    if not isinstance(y, (list, np.ndarray)): # single label -> one hot encoding\n",
    "        y = np.eye(learn.data.valid_ds.c)[y]\n",
    "\n",
    "    m = learn.model.eval()\n",
    "    xb,_ = learn.data.one_item(x)\n",
    "    xb_im = Image(learn.data.denorm(xb)[0])\n",
    "    xb = xb.cuda()\n",
    "\n",
    "    def hooked_backward(cat):\n",
    "        with hook_output(m[0]) as hook_a: \n",
    "            with hook_output(m[0], grad=True) as hook_g:\n",
    "                preds = m(xb)\n",
    "                preds[0,int(cat)].backward()\n",
    "        return hook_a,hook_g\n",
    "    def show_heatmap(img, hm, label):\n",
    "        _,axs = plt.subplots(1, 2)\n",
    "        axs[0].set_title(label)\n",
    "        img.show(axs[0])\n",
    "        axs[1].set_title(f'CAM of {label}')\n",
    "        img.show(axs[1])\n",
    "        axs[1].imshow(hm, alpha=0.6, extent=(0,img.shape[0],img.shape[0],0),\n",
    "                      interpolation='bilinear', cmap='magma');\n",
    "        plt.show()\n",
    "\n",
    "    for y_i in np.where(y > 0)[0]:\n",
    "        hook_a,hook_g = hooked_backward(cat=y_i)\n",
    "        acts = hook_a.stored[0].cpu()\n",
    "        grad = hook_g.stored[0][0].cpu()\n",
    "        grad_chan = grad.mean(1).mean(1)\n",
    "        mult = (acts*grad_chan[...,None,None]).mean(0)\n",
    "        show_heatmap(img=xb_im, hm=mult, label=str(learn.data.valid_ds.y[data_index]))\n",
    "\n",
    "for idx in range(10):\n",
    "    visualize_cnn_by_cam(learn, idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
